---
title: "Result"
author: "Ruiqi Wang"
date: "2024-05-22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
str(train_images)
# int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...
str(test_labels)
# int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

```{r}
network <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
```


```{r}
network_1 <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network_1 %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
network_1 %>% fit(train_images, train_labels, epochs = 5, batch_size = 64)
```

Here the batch size decreases from 128 to 64. The time is longer than before. Also, the best accuracy is drop from 0.91 to 0.9.

```{r}
metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics
network
```

```{r}
#Setting aside a validation set:
val_indices <- 1:10000
x_val <- train_images[val_indices,]
partial_x_train <- train_images[-val_indices,]
y_val <- train_labels[val_indices,]
partial_y_train <- train_labels[-val_indices,]
history <- network %>% fit(
partial_x_train,
partial_y_train,
epochs = 20,
batch_size = 512,
validation_data = list(x_val, y_val)
)

str(history)
plot(history)
```


```{r}
library(keras)
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
input_shape = c(28, 28, 1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_flatten() %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
model
```



```{r}
library(keras)
model_1 <- keras_model_sequential() %>%
    layer_conv_2d(filters = 32, kernel_size = c(5, 5), activation = "relu", input_shape = c(28, 28, 1), padding = "same") %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = "relu", padding = "same") %>%
    layer_max_pooling_2d(pool_size = c(2, 2)) %>%
    layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = "relu", padding = "same") %>%
    layer_flatten() %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 10, activation = "softmax")

model_1

```

Here we can see the kernel size increases from (3,3) to (5,5). The Param # in result table increases a lot. The same situation happens in total params and trainable params.



```{r}
mnist <- dataset_mnist()
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
train_images <- array_reshape(train_images, c(60000, 28, 28, 1))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
model %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
model %>% fit(
train_images, train_labels,
epochs = 5, batch_size=64
)
results <- model %>% evaluate(test_images, test_labels)
results
```